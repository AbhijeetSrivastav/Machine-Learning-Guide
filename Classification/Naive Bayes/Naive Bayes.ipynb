{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods\n",
    "\n",
    "It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.\n",
    "\n",
    "Some popular examples of Naïve Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles.\n",
    "\n",
    "### Why is it called Naïve Bayes?\n",
    "\n",
    "The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as:\n",
    "\n",
    "    Naïve: It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence\n",
    "    of other features.\n",
    "    \n",
    "    Bayes: It is called Bayes because it depends on the principle of Bayes' Theorem.\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Bayes' theorem is also known as Bayes' rule, Bayes' law, or Bayesian reasoning, which determines the probability of an event with uncertain knowledge.\n",
    "\n",
    "In probability theory, it relates the conditional probability and marginal probabilities of two random events.\n",
    "\n",
    "It is a way to calculate the value of P(B|A) with the knowledge of P(A|B).\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "\n",
    "    P(A|B) is known as posterior, which we need to calculate, and it will be read as Probability of hypothesis A when we\n",
    "    have occurred an evidence B.\n",
    "\n",
    "    P(B|A) is called the likelihood, in which we consider that hypothesis is true, then we calculate the probability \n",
    "    of evidence.\n",
    "\n",
    "    P(A) is called the prior probability, probability of hypothesis before considering the evidence\n",
    "    \n",
    "    P(B) is called marginal probability, pure probability of an evidence.\n",
    "    \n",
    "In general, we can write P (B) = P(A)*P(B|Ai), hence the Bayes' rule can be written as:\n",
    "    \n",
    "$$ P(A_{i}|B) = \\frac{P(B|A_{i})P(A_{i})}{\\Sigma^k_{i=1} P(A_{i})P(B|A_{i})} $$\n",
    "\n",
    "### Working of Naive Bayes\n",
    "\n",
    "1. Convert the given dataset into frequency tables.\n",
    "2. Generate Likelihood table by finding the probabilities of given features.\n",
    "3. Now, use Bayes theorem to calculate the posterior probability.\n",
    "\n",
    "### Limitation\n",
    "\n",
    "Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features.\n",
    "\n",
    "### Types of Naïve Bayes Model\n",
    "\n",
    "There are three types of Naive Bayes Model, which are given below:\n",
    "\n",
    "1. Gaussian: The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution.\n",
    "\n",
    "2. Multinomial: The Multinomial Naïve Bayes classifier is used when the data is multinomial distributed. It is primarily used for document classification problems, it means a particular document belongs to which category such as Sports, Politics, education, etc. The classifier uses the frequency of words for the predictors.\n",
    "\n",
    "3. Bernoulli: The Bernoulli classifier works similar to the Multinomial classifier, but the predictor variables are the independent Booleans variables. Such as if a particular word is present or not in a document. This model is also famous for document classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "Y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes Model on Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making the Naive Bayes model we are using the GaussianNB class of naive_bayes library of the sklearn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a new result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prdict method as usual is used to predict the value of target vector according to feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(sc.transform([[30,87000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Test set result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the test result we use the predict method on our trained model which requires test feature matrix and returns a predict vector consisiting of predicted value of target vector for test data.\n",
    "\n",
    "We store the predict vector in a variable which is inded the prediction of the target vector based on test feature matrix supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will concatenate the predicted value of target vector (Y_pred) and test or real value of target vector (Y_test) using the concatenate method of the numpy.\n",
    "\n",
    "Concatenate method of numpy takes a tuple containing the arrays to be merged and number of output column as argument.\n",
    "\n",
    "With that we also apply reshape method on each arrays to make them vertical insted of horizontal for better analysis.\n",
    "\n",
    "reshape method takes len of the column to be reshaped and number of output column as argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
    "\n",
    "A confusion matrix is also known as an error matrix.\n",
    "\n",
    "To make confusion matrix we use the confusion_matrix class of the metrics module of sklearn library.\n",
    "The confusion_matrix method take the test or real target vector(Y_test) and predicted target vector(Y_pred) as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the accuracy of our model in precentage we use the accuracy_score method of metrics module of sklearn library.\n",
    "\n",
    "The accuracy_score method takes real or test target vector(Y_test) and predicted target vector(Y_pred) as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Training set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, Y_set = sc.inverse_transform(X_train), Y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(Y_set)):\n",
    "    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, Y_set = sc.inverse_transform(X_test), Y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(Y_set)):\n",
    "    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
