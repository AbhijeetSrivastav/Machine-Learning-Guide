{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data into a variable which we call a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix of features:- The matrix of features is a term used in machine learning to describe the list of columns that contain independent variables to be processed, including all lines in the dataset. These lines in the dataset are called lines of observation.\n",
    "It is denoted by varaiable X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target variable vector:- The target variable vector is a term used in Machine Learning to define the list of dependent variables in the existing dataset. Here we also have lines of observations which is the list of those variables by line.\n",
    "It is denoted by Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our feature matrix and target vector we use 'iloc' method which is abbrevation for locate index.\n",
    "\n",
    "Format of iloc method is:\n",
    "\n",
    "     var = dataset.iloc[rows, columns].values\n",
    "\n",
    "And to define the rows and columns we use index operator (:), where index operator seperates start_index : end_index\n",
    "\n",
    "Index operator include start_index but exclude end_index\n",
    "\n",
    "If the index used individual it means iclude everything from 0 to last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "Y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking care of Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their are many ways of handling missing data from our feature matrix. Few of them are:\n",
    "\n",
    "1. If the dataset is too large and missing values are very less then we can  remove those values.\n",
    "\n",
    "2. We can replace the missing values with the average or mean value of that column or feature.\n",
    "\n",
    "3. We can replace the missing values with the median value of that column or feature.\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replace these missing values we use SimpleImputer class of impute module of sklearn library.\n",
    "\n",
    "1. First we create an instance or object of SimpleImputer class which expects two arguments,\n",
    "  1. Values to be replaced, i.e missing_values\n",
    "  2. Strategy to replace the missing values.\n",
    "  \n",
    "2. After that we select the rows and columns in which we have to search for missing values. And for that we use fit method on our imputer object.\n",
    "\n",
    "3. Now we choose the rows and columns in which we have to replace the missing values. And for that we use transform method in our imputer object. This returns an updated feature matrix of the selected rows and columns.\n",
    "\n",
    "4. At last we have to update the rows and columns of our original feature matrix which has been returned by the transform method after replacing the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "updated_features_of_X = imputer.transform(X[:,1:3])\n",
    "X[:, 1:3] = updated_features_of_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most machine learning models only accept numerical variables, preprocessing the categorical variables becomes a necessary step. \n",
    "\n",
    "We need to convert these categorical variables to numbers such that the model is able to understand and extract valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables are usually represented as ‘strings’ or ‘categories’ and are finite in number. \n",
    "\n",
    "Further, we can see there are two kinds of categorical data-\n",
    "\n",
    "    1. Ordinal Data: The categories have an inherent order\n",
    "    2. Nominal Data: The categories do not have an inherent order\n",
    "    \n",
    "In Ordinal data, while encoding, one should retain the information regarding the order in which the category is provided.\n",
    "In most of the cases ordinal data are independent variables.\n",
    "Example- Countries in the given data\n",
    "\n",
    "While encoding Nominal data, we have to consider the presence or absence of a feature. In such a case, no notion of order is present. \n",
    "In most of the cased Nominal data are dependent variables\n",
    "Example- Purchased in given data\n",
    "\n",
    "To encode the Ordinal Data we use method called OneHot Encoding.\n",
    "\n",
    "And to encode the Nominal Data we use method called Label Encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Independent Variable or Ordinal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode independent variable or ordinal data we use ColumnTransformer class of compose module of sklearn library and also OneHotEncoder class of preprocessing module of sklearn library\n",
    "\n",
    "1. First we create an object or instance of ColumnTransformer class which expect two arguments\n",
    "    A. column to be transformed and encoder to be used specified within a tuple, i.e transformers\n",
    "    B. wheteher we want to retain the columns not being transformed or not, i.e remainder\n",
    "    \n",
    "2. After that we connect our ColumnTransformer object to the feature matrix or independent variable and encode the categorical values specified.\n",
    "\n",
    "3. Future machine learning model requires numpy array for further processing so we forcefully convert our encoded feature matrix to numpy array and update our feature matrix with updated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(),[0])], remainder='passthrough')\n",
    "encoded_feature_matrix = ct.fit_transform(X)\n",
    "X = np.array(encoded_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Dependent Variable or Nominal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode dependent variable or nominal data we use LabelEncoder class of preprocessing module of sklearn library\n",
    "\n",
    "1. First we create an object or instance of LabelEncoder class which expect no arguments\n",
    "    \n",
    "2. After that we connect our LabelEncoder object to the target vector or dependent variable and encode the categorical values specified.\n",
    "\n",
    "3. It is not necessary to convert target variable to numpy array thus we only update our target vector with updated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "encoded_target_variable = le.fit_transform(Y)\n",
    "Y = encoded_target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the dataset into Training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train-test split is a technique for evaluating the performance of a machine learning algorithm.\n",
    "\n",
    "The procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset.\n",
    "\n",
    "    Train Dataset: Used to fit the machine learning model.\n",
    "    Test Dataset: Used to evaluate the fit machine learning model.\n",
    "    \n",
    "Each dataset is divided into two subsets, onse is feature matrix and other is target vector.\n",
    "\n",
    "Theirfore after spltting the dataset we have four sets which are:\n",
    "    1. Training feature matrix, i.e X_train\n",
    "    2. Test feature matrix, i.e X_test\n",
    "    3. Training target vector, i.e Y_train\n",
    "    4. Testing target vector, i.e Y_test\n",
    "    \n",
    "The objective is to estimate the performance of the machine learning model on new data: data not used to train the model.\n",
    "\n",
    "This is how we expect to use the model in practice. Namely, to fit it on available data with known inputs and outputs, then make predictions on new examples in the future where we do not have the expected output or target values.\n",
    "\n",
    "An optimal splitting size is 80% training data and 20% test data and may vary according to conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the data we use the train_test_split class of the model_selection module of the sklearn library\n",
    "\n",
    "The train_test_split class returns for datasets and require three arguments\n",
    "    1. Feature matrix, i.e X\n",
    "    2. Target vector, i.e Y\n",
    "    3. Test size percentage, i.test_size\n",
    "    3. Randomnes of split, i.e random_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19159184 -1.07812594]\n",
      " [-0.01411729 -0.07013168]\n",
      " [ 0.56670851  0.63356243]\n",
      " [-0.30453019 -0.30786617]\n",
      " [-1.90180114 -1.42046362]\n",
      " [ 1.14753431  1.23265336]\n",
      " [ 1.43794721  1.57499104]\n",
      " [-0.74014954 -0.56461943]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is a method used to normalize the range of independent variables or feature matrix.\n",
    "Example- If the difference between the values in feature matrix is too high like some values are between 100 to 200 and others are 1000 to 100000.\n",
    "\n",
    "In data processing, it is also known as data normalization and is generally performed during the data preprocessing\n",
    "step.\n",
    "\n",
    "Few advantages of normalizing the data are as follows:\n",
    "\n",
    "    1. It makes your training faster.\n",
    "    2. It prevents you from getting stuck in local optima.\n",
    "    3. It gives you a better error surface shape.\n",
    "    4. Wweight decay and bayes optimization can be done more conveniently.\n",
    "    \n",
    "Feature scaling is done after spliting the dataset in training and test subsets because the methods used for scaling require the mean of the feature from the feature matrix.\n",
    "\n",
    "Thus if we scale before the spliting mean of the features would be mean of the features from train + test data but as test data is supposed to be a new data which our model will get it in real time to predict it is done after spliting.\n",
    "\n",
    "If it is done before spliting then the condition of overfiting will occur.\n",
    "\n",
    "In scaling we scale our training feature matrix X_train and as well as we scale our test feature matrix X_test.\n",
    "But to scale our test feature matrix we use the same scaler that we used for the training feature matrix because we have to predict based on training.\n",
    "Using of the same scaler lead to use of the same calculate mean, max and min used in the formulas of different methods used for scaling the training set.\n",
    "\n",
    "Feature scaling is not done for the dummy features or the features formed during the encoding of the categorical data as they dont affect the performance too much.\n",
    "\n",
    "\n",
    "Their are two methods for scaling the feature matrix:\n",
    "    \n",
    "       1. Normalization:- Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n",
    "    \n",
    "    Formula is, \n",
    "    \n",
    "    Xnorm = X - min(X)  / max(X) - min(X)\n",
    "    \n",
    "    where \n",
    "    Xnorm is normalized value range between 0 and 1\n",
    "    X is feature \n",
    "    min(X) is minimum of the feature or column\n",
    "    max(X) is maximum of the feature or column\n",
    "    \n",
    "       2. Standardization:- Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n",
    "    \n",
    "    Formula is,\n",
    "   \n",
    "       Xstandard = X -mean(X) / standard deviation(X)\n",
    "    \n",
    "    where\n",
    "    Xstandard is standardized value range bewtween -3 and 3\n",
    "    X is feature\n",
    "    standard deviadtion(X) is standard deviation of feature\n",
    "    \n",
    "\n",
    "Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
    "\n",
    "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n",
    "        \n",
    "But in most cases Standardization is prefered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale the feature matrix we use the StandardScaler class of the preprocessing module of the sklearn library.\n",
    "\n",
    "1. First we create the object or instance of the StandardScaler class.\n",
    "2. Then we scale the selected features of the X_train or training feature matrix in which we exclude the dummy features using fit_transform method.\n",
    "3. Now we update the X_train feature matrix with the scaled values.\n",
    "4. Now we scale the X_train or test feature matrix using the same scaler object thus only using the transform method to scale and not the fit_transform which will lead to creation of new scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "scaled_feature_train = sc.fit_transform(X_train[:, 3:5])\n",
    "X_train[:, 3:5] = scaled_feature_train\n",
    "\n",
    "scaled_feature_test = sc.transform(X_test[:, 3:5])\n",
    "X_test[:, 3:5] = scaled_feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
