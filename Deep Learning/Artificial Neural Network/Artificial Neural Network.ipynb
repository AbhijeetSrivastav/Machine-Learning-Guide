{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:-1].values\n",
    "Y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode dependent variable or nominal data we use LabelEncoder class of preprocessing module of sklearn library\n",
    "\n",
    "1. First we create an object or instance of LabelEncoder class which expect no arguments\n",
    "\n",
    "2. After that we connect our LabelEncoder object to the target vector or dependent variable and encode the categorical values specified.\n",
    "\n",
    "3. It is not necessary to convert target variable to numpy array thus we only update our target vector with updated one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding the Gender Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "encoded_target_column = le.fit_transform(X[:, 2])\n",
    "X[:, 2] = encoded_target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding the Geography Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode independent variable or ordinal data we use ColumnTransformer class of compose module of sklearn library and also OneHotEncoder class of preprocessing module of sklearn library\n",
    "\n",
    "1. First we create an object or instance of ColumnTransformer class which expect two arguments A. column to be transformed and encoder to be used specified within a tuple, i.e transformers B. wheteher we want to retain the columns not being transformed or not, i.e remainder\n",
    "\n",
    "2. After that we connect our ColumnTransformer object to the feature matrix or independent variable and encode the categorical values specified.\n",
    "\n",
    "3. Future machine learning model requires numpy array for further processing so we forcefully convert our encoded feature matrix to numpy array and update our feature matrix with updated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(),[1])], remainder='passthrough')\n",
    "encoded_feature_matrix = ct.fit_transform(X)\n",
    "X = np.array(encoded_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
      " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
      " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 ... 0 1 124749.08]\n",
      " [1.0 0.0 0.0 ... 0 0 41104.82]\n",
      " [0.0 1.0 0.0 ... 1 1 45750.21]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 1 1 92027.69]\n",
      " [1.0 0.0 0.0 ... 1 1 101168.9]\n",
      " [0.0 1.0 0.0 ... 1 0 33462.94]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 97057.28]\n",
      " [1.0 0.0 0.0 ... 1 0 66526.01]\n",
      " [1.0 0.0 0.0 ... 0 1 90537.47]\n",
      " ...\n",
      " [0.0 0.0 1.0 ... 0 1 161571.79]\n",
      " [0.0 1.0 0.0 ... 1 1 165257.31]\n",
      " [0.0 1.0 0.0 ... 1 1 49025.79]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale the feature matrix we use the StandardScaler class of the preprocessing module of the sklearn library.\n",
    "\n",
    "1. First we create the object or instance of the StandardScaler class.\n",
    "\n",
    "2. Then we scale the selected features of the X_train or training feature matrix in which we exclude the dummy features \n",
    "   using fit_transform method.\n",
    "\n",
    "3. Now we update the X_train feature matrix with the scaled values.\n",
    "\n",
    "4. Now we scale the X_train or test feature matrix using the same scaler object thus only using the transform method to scale \n",
    "   and not the fit_transform which will lead to creation of new scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "scaled_feature_train = sc.fit_transform(X_train)\n",
    "X_train = scaled_feature_train\n",
    "\n",
    "scaled_feature_test = sc.transform(X_test)\n",
    "X_test = scaled_feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99850112  1.71490137 -0.57273139 ... -1.55337352  0.97725852\n",
      "   0.42739449]\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ... -1.55337352 -1.02327069\n",
      "  -1.02548708]\n",
      " [-0.99850112  1.71490137 -0.57273139 ...  0.64376017  0.97725852\n",
      "  -0.94479772]\n",
      " ...\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ...  0.64376017  0.97725852\n",
      "  -0.14096853]\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ...  0.64376017  0.97725852\n",
      "   0.01781218]\n",
      " [-0.99850112  1.71490137 -0.57273139 ...  0.64376017 -1.02327069\n",
      "  -1.15822478]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00150113 -0.58312392 -0.57273139 ...  0.64376017  0.97725852\n",
      "  -0.05360571]\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ...  0.64376017 -1.02327069\n",
      "  -0.58392685]\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ... -1.55337352  0.97725852\n",
      "  -0.16685331]\n",
      " ...\n",
      " [-0.99850112 -0.58312392  1.74601919 ... -1.55337352  0.97725852\n",
      "   1.0669965 ]\n",
      " [-0.99850112  1.71490137 -0.57273139 ...  0.64376017  0.97725852\n",
      "   1.13101314]\n",
      " [-0.99850112  1.71490137 -0.57273139 ...  0.64376017  0.97725852\n",
      "  -0.88790165]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our Artifcial Neural Networ we create an object or instance of Sequential class which belogs to models sub module of keras module of tensorflow library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann =  tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding input layer and first hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add layer to our ANN we are using the add method of tensorflow library.\n",
    "\n",
    "add method take as argument object or instance of Dense class which belongs to layers sub module of keras module of tensorflow library.\n",
    "\n",
    "This object of Dense class takes as argument:\n",
    "\n",
    "    number of neurons (units)\n",
    "    \n",
    "    activation function (relu)\n",
    "    \n",
    "Selection of number of neuron is experimental and their is no rule to decide it in advance.\n",
    "\n",
    "Activation function is selected based on the operation we want to perform within the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the Second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput layer is added similar to the others layer.\n",
    "\n",
    "Here we have choosen units or number of neurons in ouput layer as one because our dependent variable (Exited) is binary and it may either be 1 or 0 and for activation function we have choosen sigmoid as it returns binary ouput as well as probabilty of getting that binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile our ANN we are going to use compile method of tensorflow which take as argument:\n",
    "    \n",
    "    optimizer: a method which optimizes the weight on edge between neurons of two layers with aim of reducing the loss\n",
    "    \n",
    "    loss: loss is difference between predicted and actual value\n",
    "    \n",
    "    metrics: is the array which takes different parameters which are used to measure the efficicency of ANN\n",
    "    \n",
    "In this case we are using 'adam' optimizer fuction which is an Stochastic Gradient Descent and 'binary_crossentropy' as our loss funciton and we are just measuring the accuracy of the ANN .\n",
    "\n",
    "In this case we are basically classifying user wheter he exited or not thus binary classification and binary_crossentropy loss fucnction.\n",
    "\n",
    "If we have multiple classes to classify we will use 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on  Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our ANN we are using fit method of sklearn which takes as agument feature train matrix and target train vector as well as the size of batch we are training and iterations or epochs or number of iter to train.\n",
    "\n",
    "In ANN we dont traint the model on whole dataset at once instead we supply the model with batch of training data and its values is experimental but in most of the cases 32 suits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7765\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7972\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4460 - accuracy: 0.7972\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.7972\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.7972\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8035\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.8112\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8138\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4232 - accuracy: 0.8176\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8191\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4174 - accuracy: 0.8230\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8245\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.8299\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8316\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8332\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8332\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4040 - accuracy: 0.8340\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.8336\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8354\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8354\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8360\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8370\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8369\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8353\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8363\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8369\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8380\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8386\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8389\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8384\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8390\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8382\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8389\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8388\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8391\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.8386\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8382\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8382\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8382\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8388\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8405\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8380\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8384\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3919 - accuracy: 0.8393\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8395\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8384\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3910 - accuracy: 0.8389\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8372\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3910 - accuracy: 0.8401\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8394\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8390\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8385\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8394\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.8381\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8394\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3903 - accuracy: 0.8406\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8395\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8400\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8397\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8394\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8416\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8394\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3903 - accuracy: 0.8403\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8400\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8403\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8409\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8407\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8405\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.8395\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8405\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8401\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8411\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8409\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8409\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8409\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8390\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8411\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8414\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8405\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8405\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8403\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8393\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8389\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8407\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8409\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8409\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8397\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8403\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8382\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8401\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.8395\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8403\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8415\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8400\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8405\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3891 - accuracy: 0.8404\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8410\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8401\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8399\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11175c8fa30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, Y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Prediction and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the result of Single Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to predict that whether a customer will exit the bank or not based on the input given below.\n",
    "\n",
    "*Geography: France*\n",
    "\n",
    "*Credit Score: 600*\n",
    "\n",
    "*Gender: Male*\n",
    "\n",
    "*Age: 40 years old*\n",
    "\n",
    "*Tenure: 3 years*\n",
    "\n",
    "*Balance: \\$ 60000*\n",
    "\n",
    "*Number of Products: 2*\n",
    "\n",
    "*Does this customer have a credit card ? Yes*\n",
    "\n",
    "*Is this customer an Active Member: Yes*\n",
    "\n",
    "*Estimated Salary: \\$ 50000*\n",
    "\n",
    "So, should we say goodbye to that customer ?\n",
    "\n",
    "To predict that we are going to use the predict method and also use sc.transform method to transform values back to original scalae which were alterd in feature scaling step and after that comparing it with 0.5 which means if the value is greater than 0.5 its 'TRUE' else 'FALSE'.\n",
    "\n",
    "**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n",
    "\n",
    "**Important note 2:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the test result we use the predict method on our trained model which requires test feature matrix and returns a predict vector consisiting of predicted value of target vector for test data.\n",
    "\n",
    "We store the predict vector in a variable which is inded the prediction of the target vector based on test feature matrix supplied.\n",
    "\n",
    "Now we will concatenate the predicted value of target vector (Y_pred) and test or real value of target vector (Y_test) using the concatenate method of the numpy.\n",
    "\n",
    "Concatenate method of numpy takes a tuple containing the arrays to be merged and number of output column as argument.\n",
    "\n",
    "With that we also apply reshape method on each arrays to make them vertical insted of horizontal for better analysis.\n",
    "\n",
    "reshape method takes len of the column to be reshaped and number of output column as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = ann.predict(X_test)\n",
    "Y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
    "\n",
    "A confusion matrix is also known as an error matrix.\n",
    "\n",
    "To make confusion matrix we use the confusion_matrix class of the metrics module of sklearn library. The confusion_matrix method take the test or real target vector(Y_test) and predicted target vector(Y_pred) as argument.\n",
    "\n",
    "To get the accuracy of our model in precentage we use the accuracy_score method of metrics module of sklearn library.\n",
    "\n",
    "The accuracy_score method takes real or test target vector(Y_test) and predicted target vector(Y_pred) as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1526   59]\n",
      " [ 272  143]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8345"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8345"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
