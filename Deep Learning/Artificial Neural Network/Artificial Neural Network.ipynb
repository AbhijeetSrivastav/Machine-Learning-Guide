{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "An Artificial neural network is usually a computational network based on biological neural networks that construct the structure of the human brain. Similar to a human brain has neurons interconnected to each other, artificial neural networks also have neurons that are linked to each other in various layers of the networks.\n",
    "\n",
    "![Neural Network Biological](neuronbio.png)\n",
    "\n",
    "#### Analogy of ANN with Biological Neural Network\n",
    "\n",
    "Relationship between Biological neural network and artificial neural network:\n",
    "\n",
    "    Biological Neural Network => Artificial Neural Network\n",
    "    \n",
    "    Dendrite => Inputs\n",
    "    \n",
    "    Cell nucleus => Nodes\n",
    "    \n",
    "    Synapse => Weights\n",
    "    \n",
    "    Axon =>\tOutput\n",
    "\n",
    "### Architecture of ANN\n",
    "\n",
    "To understand the concept of the architecture of an artificial neural network, we have to understand what a neural network consists of. In order to define a neural network that consists of a large number of artificial neurons, which are termed units arranged in a sequence of layers. Lets us look at various types of layers available in an artificial neural network.\n",
    "\n",
    "![ANN](ann.png)\n",
    "\n",
    "Artificial Neural Network primarily consists of three layers:\n",
    "\n",
    "**Input Layer:**\n",
    "\n",
    "As the name suggests, it accepts inputs in several different formats provided by the programmer.\n",
    "\n",
    "**Hidden Layer:**\n",
    "\n",
    "The hidden layer presents in-between input and output layers. It performs all the calculations to find hidden features and patterns.\n",
    "\n",
    "**Output Layer:**\n",
    "\n",
    "The input goes through a series of transformations using the hidden layer, which finally results in output that is conveyed using this layer.\n",
    "\n",
    "#### Working of ANN\n",
    "\n",
    "![ANN Deep](annd.png)\n",
    "\n",
    "Artificial Neural Network can be best represented as a weighted directed graph, where the artificial neurons form the nodes. The association between the neurons outputs and neuron inputs can be viewed as the directed edges with weights. The Artificial Neural Network receives the input signal from the external source in the form of a pattern and image in the form of a vector. These inputs are then mathematically assigned by the notations x(n) for every n number of inputs.\n",
    "\n",
    "\n",
    "Afterward, each of the input is multiplied by its corresponding weights ( these weights are the details utilized by the artificial neural networks to solve a specific problem ). In general terms, these weights normally represent the strength of the interconnection between neurons inside the artificial neural network. All the weighted inputs are summarized inside the computing unit.\n",
    "\n",
    "If the weighted sum is equal to zero, then bias is added to make the output non-zero or something else to scale up to the system's response. Bias has the same input, and weight equals to 1. Here the total of weighted inputs can be in the range of 0 to positive infinity. Here, to keep the response in the limits of the desired value, a certain maximum value is benchmarked, and the total of weighted inputs is passed through the activation function.\n",
    "\n",
    "The activation function refers to the set of transfer functions used to achieve the desired output. There is a different kind of the activation function, but primarily either linear or non-linear sets of functions.\n",
    "\n",
    "**In Short**\n",
    "\n",
    "The artificial neural network takes input and computes the weighted sum of the inputs and includes a bias. This computation is represented in the form of a transfer function.\n",
    "\n",
    "$$ \\sum_{i = 1}^{n} W_{i} * X_{i} + b $$\n",
    "\n",
    "It determines weighted total is passed as an input to an activation function to produce the output. Activation functions choose whether a node should fire or not. Only those who are fired make it to the output layer. There are distinctive activation functions available that can be applied upon the sort of task we are performing.\n",
    "\n",
    "\n",
    "#### Activation Function\n",
    "\n",
    "\n",
    "An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network.\n",
    "\n",
    "Sometimes the activation function is called a “transfer function.” If the output range of the activation function is limited, then it may be called a “squashing function.” Many activation functions are nonlinear and may be referred to as the “nonlinearity” in the layer or the network design.\n",
    "\n",
    "The choice of activation function has a large impact on the capability and performance of the neural network, and different activation functions may be used in different parts of the model\n",
    "\n",
    "Read in detail about activation function [here](https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/)\n",
    "\n",
    "#### Types of Artificial Neural Network\n",
    "\n",
    "There are various types of Artificial Neural Networks (ANN) depending upon the human brain neuron and network functions, an artificial neural network similarly performs tasks. The majority of the artificial neural networks will have some similarities with a more complex biological partner and are very effective at their expected tasks. For example, segmentation or classification.\n",
    "\n",
    "1. **Feedback ANN:**\n",
    "\n",
    "In this type of ANN, the output returns into the network to accomplish the best-evolved results internally. As per the University of Massachusetts, Lowell Centre for Atmospheric Research. The feedback networks feed information back into itself and are well suited to solve optimization issues. The Internal system error corrections utilize feedback ANNs.\n",
    "\n",
    "2. **Feed-Forward ANN:**\n",
    "\n",
    "A feed-forward network is a basic neural network comprising of an input layer, an output layer, and at least one layer of a neuron. Through assessment of its output by reviewing its input, the intensity of the network can be noticed based on group behavior of the associated neurons, and the output is decided. The primary advantage of this network is that it figures out how to evaluate and recognize input patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:-1].values\n",
    "Y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode dependent variable or nominal data we use LabelEncoder class of preprocessing module of sklearn library\n",
    "\n",
    "1. First we create an object or instance of LabelEncoder class which expect no arguments\n",
    "\n",
    "2. After that we connect our LabelEncoder object to the target vector or dependent variable and encode the categorical values specified.\n",
    "\n",
    "3. It is not necessary to convert target variable to numpy array thus we only update our target vector with updated one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding the Gender Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "encoded_target_column = le.fit_transform(X[:, 2])\n",
    "X[:, 2] = encoded_target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding the Geography Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode independent variable or ordinal data we use ColumnTransformer class of compose module of sklearn library and also OneHotEncoder class of preprocessing module of sklearn library\n",
    "\n",
    "1. First we create an object or instance of ColumnTransformer class which expect two arguments A. column to be transformed and encoder to be used specified within a tuple, i.e transformers B. wheteher we want to retain the columns not being transformed or not, i.e remainder\n",
    "\n",
    "2. After that we connect our ColumnTransformer object to the feature matrix or independent variable and encode the categorical values specified.\n",
    "\n",
    "3. Future machine learning model requires numpy array for further processing so we forcefully convert our encoded feature matrix to numpy array and update our feature matrix with updated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(),[1])], remainder='passthrough')\n",
    "encoded_feature_matrix = ct.fit_transform(X)\n",
    "X = np.array(encoded_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
      " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
      " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 ... 0 1 124749.08]\n",
      " [1.0 0.0 0.0 ... 0 0 41104.82]\n",
      " [0.0 1.0 0.0 ... 1 1 45750.21]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 1 1 92027.69]\n",
      " [1.0 0.0 0.0 ... 1 1 101168.9]\n",
      " [0.0 1.0 0.0 ... 1 0 33462.94]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 97057.28]\n",
      " [1.0 0.0 0.0 ... 1 0 66526.01]\n",
      " [1.0 0.0 0.0 ... 0 1 90537.47]\n",
      " ...\n",
      " [0.0 0.0 1.0 ... 0 1 161571.79]\n",
      " [0.0 1.0 0.0 ... 1 1 165257.31]\n",
      " [0.0 1.0 0.0 ... 1 1 49025.79]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale the feature matrix we use the StandardScaler class of the preprocessing module of the sklearn library.\n",
    "\n",
    "1. First we create the object or instance of the StandardScaler class.\n",
    "\n",
    "2. Then we scale the selected features of the X_train or training feature matrix in which we exclude the dummy features \n",
    "   using fit_transform method.\n",
    "\n",
    "3. Now we update the X_train feature matrix with the scaled values.\n",
    "\n",
    "4. Now we scale the X_train or test feature matrix using the same scaler object thus only using the transform method to scale \n",
    "   and not the fit_transform which will lead to creation of new scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "scaled_feature_train = sc.fit_transform(X_train)\n",
    "X_train = scaled_feature_train\n",
    "\n",
    "scaled_feature_test = sc.transform(X_test)\n",
    "X_test = scaled_feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99850112  1.71490137 -0.57273139 ... -1.55337352  0.97725852\n",
      "   0.42739449]\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ... -1.55337352 -1.02327069\n",
      "  -1.02548708]\n",
      " [-0.99850112  1.71490137 -0.57273139 ...  0.64376017  0.97725852\n",
      "  -0.94479772]\n",
      " ...\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ...  0.64376017  0.97725852\n",
      "  -0.14096853]\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ...  0.64376017  0.97725852\n",
      "   0.01781218]\n",
      " [-0.99850112  1.71490137 -0.57273139 ...  0.64376017 -1.02327069\n",
      "  -1.15822478]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00150113 -0.58312392 -0.57273139 ...  0.64376017  0.97725852\n",
      "  -0.05360571]\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ...  0.64376017 -1.02327069\n",
      "  -0.58392685]\n",
      " [ 1.00150113 -0.58312392 -0.57273139 ... -1.55337352  0.97725852\n",
      "  -0.16685331]\n",
      " ...\n",
      " [-0.99850112 -0.58312392  1.74601919 ... -1.55337352  0.97725852\n",
      "   1.0669965 ]\n",
      " [-0.99850112  1.71490137 -0.57273139 ...  0.64376017  0.97725852\n",
      "   1.13101314]\n",
      " [-0.99850112  1.71490137 -0.57273139 ...  0.64376017  0.97725852\n",
      "  -0.88790165]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our Artifcial Neural Networ we create an object or instance of Sequential class which belogs to models sub module of keras module of tensorflow library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann =  tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding input layer and first hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add layer to our ANN we are using the add method of tensorflow library.\n",
    "\n",
    "add method take as argument object or instance of Dense class which belongs to layers sub module of keras module of tensorflow library.\n",
    "\n",
    "This object of Dense class takes as argument:\n",
    "\n",
    "    number of neurons (units)\n",
    "    \n",
    "    activation function (relu)\n",
    "    \n",
    "Selection of number of neuron is experimental and their is no rule to decide it in advance.\n",
    "\n",
    "Activation function is selected based on the operation we want to perform within the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the Second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput layer is added similar to the others layer.\n",
    "\n",
    "Here we have choosen units or number of neurons in ouput layer as one because our dependent variable (Exited) is binary and it may either be 1 or 0 and for activation function we have choosen sigmoid as it returns binary ouput as well as probabilty of getting that binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile our ANN we are going to use compile method of tensorflow which take as argument:\n",
    "    \n",
    "    optimizer: a method which optimizes the weight on edge between neurons of two layers with aim of reducing the loss\n",
    "    \n",
    "    loss: loss is difference between predicted and actual value\n",
    "    \n",
    "    metrics: is the array which takes different parameters which are used to measure the efficicency of ANN\n",
    "    \n",
    "In this case we are using 'adam' optimizer fuction which is an Stochastic Gradient Descent and 'binary_crossentropy' as our loss funciton and we are just measuring the accuracy of the ANN .\n",
    "\n",
    "In this case we are basically classifying user wheter he exited or not thus binary classification and binary_crossentropy loss fucnction.\n",
    "\n",
    "If we have multiple classes to classify we will use 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on  Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our ANN we are using fit method of sklearn which takes as agument feature train matrix and target train vector as well as the size of batch we are training and iterations or epochs or number of iter to train.\n",
    "\n",
    "In ANN we dont traint the model on whole dataset at once instead we supply the model with batch of training data and its values is experimental but in most of the cases 32 suits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 0s 800us/step - loss: 0.5419 - accuracy: 0.7784\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 867us/step - loss: 0.4695 - accuracy: 0.7971\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 833us/step - loss: 0.4470 - accuracy: 0.8002\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 840us/step - loss: 0.4358 - accuracy: 0.8134\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 860us/step - loss: 0.4281 - accuracy: 0.8173\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 830us/step - loss: 0.4209 - accuracy: 0.8217\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 842us/step - loss: 0.4145 - accuracy: 0.8239\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 834us/step - loss: 0.4084 - accuracy: 0.8288\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.4031 - accuracy: 0.8305\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 842us/step - loss: 0.3987 - accuracy: 0.8313\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 837us/step - loss: 0.3946 - accuracy: 0.8324\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 853us/step - loss: 0.3909 - accuracy: 0.8335\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 857us/step - loss: 0.3868 - accuracy: 0.8354\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 847us/step - loss: 0.3823 - accuracy: 0.8413\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 862us/step - loss: 0.3781 - accuracy: 0.8425\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 855us/step - loss: 0.3744 - accuracy: 0.8457\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 951us/step - loss: 0.3710 - accuracy: 0.84800s - loss: 0.3690 - accuracy: 0.\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 949us/step - loss: 0.3681 - accuracy: 0.84850s - loss: 0.3767 - accuracy: \n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 989us/step - loss: 0.3655 - accuracy: 0.8506\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3627 - accuracy: 0.8526\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8505\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3587 - accuracy: 0.8539\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8535\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 908us/step - loss: 0.3556 - accuracy: 0.8551\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 874us/step - loss: 0.3540 - accuracy: 0.8556\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 881us/step - loss: 0.3531 - accuracy: 0.8570\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 897us/step - loss: 0.3526 - accuracy: 0.8556\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 849us/step - loss: 0.3514 - accuracy: 0.8577\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 894us/step - loss: 0.3505 - accuracy: 0.8583\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 868us/step - loss: 0.3499 - accuracy: 0.8576\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 864us/step - loss: 0.3493 - accuracy: 0.8579\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 928us/step - loss: 0.3490 - accuracy: 0.8587\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 884us/step - loss: 0.3483 - accuracy: 0.8589\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 877us/step - loss: 0.3483 - accuracy: 0.8577\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 957us/step - loss: 0.3475 - accuracy: 0.8574\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 899us/step - loss: 0.3472 - accuracy: 0.8590\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 908us/step - loss: 0.3472 - accuracy: 0.8589\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 921us/step - loss: 0.3465 - accuracy: 0.8600\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 953us/step - loss: 0.3466 - accuracy: 0.8586\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 925us/step - loss: 0.3460 - accuracy: 0.8596\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 861us/step - loss: 0.3456 - accuracy: 0.8594\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 854us/step - loss: 0.3453 - accuracy: 0.85850s - loss: 0.3373 - accuracy: \n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 837us/step - loss: 0.3453 - accuracy: 0.8590\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 866us/step - loss: 0.3450 - accuracy: 0.8602\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 877us/step - loss: 0.3452 - accuracy: 0.8595\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 917us/step - loss: 0.3442 - accuracy: 0.8597\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 860us/step - loss: 0.3447 - accuracy: 0.8600\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 856us/step - loss: 0.3441 - accuracy: 0.8596\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 0.3442 - accuracy: 0.8615\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.3439 - accuracy: 0.8604\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.3436 - accuracy: 0.86190s - loss: 0.3447 - accuracy: 0.86\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 890us/step - loss: 0.3433 - accuracy: 0.8599\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 910us/step - loss: 0.3433 - accuracy: 0.85970s - loss: 0.3452 - accuracy: 0.\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 864us/step - loss: 0.3434 - accuracy: 0.8614\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 843us/step - loss: 0.3430 - accuracy: 0.8593\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 862us/step - loss: 0.3426 - accuracy: 0.8619\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 893us/step - loss: 0.3428 - accuracy: 0.8595\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.3428 - accuracy: 0.8608\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.3426 - accuracy: 0.8605\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 897us/step - loss: 0.3430 - accuracy: 0.8604\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 936us/step - loss: 0.3421 - accuracy: 0.8625\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 925us/step - loss: 0.3424 - accuracy: 0.8601\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 846us/step - loss: 0.3418 - accuracy: 0.8616\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 895us/step - loss: 0.3416 - accuracy: 0.8609\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.3413 - accuracy: 0.8609\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 894us/step - loss: 0.3409 - accuracy: 0.8606\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 890us/step - loss: 0.3407 - accuracy: 0.8619\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 927us/step - loss: 0.3406 - accuracy: 0.8612\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 944us/step - loss: 0.3414 - accuracy: 0.8601\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 916us/step - loss: 0.3400 - accuracy: 0.8600\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 856us/step - loss: 0.3399 - accuracy: 0.8618\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 859us/step - loss: 0.3396 - accuracy: 0.8625\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 848us/step - loss: 0.3390 - accuracy: 0.8619\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 887us/step - loss: 0.3395 - accuracy: 0.8615\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.3394 - accuracy: 0.8615\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 865us/step - loss: 0.3389 - accuracy: 0.8619\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 870us/step - loss: 0.3383 - accuracy: 0.8612\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 889us/step - loss: 0.3385 - accuracy: 0.8618\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 876us/step - loss: 0.3382 - accuracy: 0.8629\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 831us/step - loss: 0.3383 - accuracy: 0.8616\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 839us/step - loss: 0.3381 - accuracy: 0.86180s - loss: 0.3398 - accuracy: 0.\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 861us/step - loss: 0.3373 - accuracy: 0.8615\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 845us/step - loss: 0.3375 - accuracy: 0.8627\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 851us/step - loss: 0.3370 - accuracy: 0.8618\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 852us/step - loss: 0.3372 - accuracy: 0.8629\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 834us/step - loss: 0.3376 - accuracy: 0.8622\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 842us/step - loss: 0.3368 - accuracy: 0.8629\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 838us/step - loss: 0.3370 - accuracy: 0.8616\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 828us/step - loss: 0.3369 - accuracy: 0.8618\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 830us/step - loss: 0.3370 - accuracy: 0.8633\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 844us/step - loss: 0.3366 - accuracy: 0.8620\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 836us/step - loss: 0.3365 - accuracy: 0.8626\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 832us/step - loss: 0.3360 - accuracy: 0.8633\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 853us/step - loss: 0.3364 - accuracy: 0.8614\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 837us/step - loss: 0.3363 - accuracy: 0.8625\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 842us/step - loss: 0.3360 - accuracy: 0.8619\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 837us/step - loss: 0.3359 - accuracy: 0.8627\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 841us/step - loss: 0.3353 - accuracy: 0.8634\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 835us/step - loss: 0.3357 - accuracy: 0.8635\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 829us/step - loss: 0.3356 - accuracy: 0.8635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2abe2b445b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, Y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Prediction and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the result of Single Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to predict that whether a customer will exit the bank or not based on the input given below.\n",
    "\n",
    "*Geography: France*\n",
    "\n",
    "*Credit Score: 600*\n",
    "\n",
    "*Gender: Male*\n",
    "\n",
    "*Age: 40 years old*\n",
    "\n",
    "*Tenure: 3 years*\n",
    "\n",
    "*Balance: \\$ 60000*\n",
    "\n",
    "*Number of Products: 2*\n",
    "\n",
    "*Does this customer have a credit card ? Yes*\n",
    "\n",
    "*Is this customer an Active Member: Yes*\n",
    "\n",
    "*Estimated Salary: \\$ 50000*\n",
    "\n",
    "So, should we say goodbye to that customer ?\n",
    "\n",
    "To predict that we are going to use the predict method and also use sc.transform method to transform values back to original scalae which were alterd in feature scaling step and after that comparing it with 0.5 which means if the value is greater than 0.5 its 'TRUE' else 'FALSE'.\n",
    "\n",
    "**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n",
    "\n",
    "**Important note 2:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the test result we use the predict method on our trained model which requires test feature matrix and returns a predict vector consisiting of predicted value of target vector for test data.\n",
    "\n",
    "We store the predict vector in a variable which is inded the prediction of the target vector based on test feature matrix supplied.\n",
    "\n",
    "Now we will concatenate the predicted value of target vector (Y_pred) and test or real value of target vector (Y_test) using the concatenate method of the numpy.\n",
    "\n",
    "Concatenate method of numpy takes a tuple containing the arrays to be merged and number of output column as argument.\n",
    "\n",
    "With that we also apply reshape method on each arrays to make them vertical insted of horizontal for better analysis.\n",
    "\n",
    "reshape method takes len of the column to be reshaped and number of output column as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = ann.predict(X_test)\n",
    "Y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
    "\n",
    "A confusion matrix is also known as an error matrix.\n",
    "\n",
    "To make confusion matrix we use the confusion_matrix class of the metrics module of sklearn library. The confusion_matrix method take the test or real target vector(Y_test) and predicted target vector(Y_pred) as argument.\n",
    "\n",
    "To get the accuracy of our model in precentage we use the accuracy_score method of metrics module of sklearn library.\n",
    "\n",
    "The accuracy_score method takes real or test target vector(Y_test) and predicted target vector(Y_pred) as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1526   59]\n",
      " [ 272  143]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8345"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8345"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
