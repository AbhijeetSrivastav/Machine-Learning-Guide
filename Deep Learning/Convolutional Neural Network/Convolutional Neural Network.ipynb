{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics.\n",
    "The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area.\n",
    "\n",
    "![CNN](cnn.jpeg)\n",
    "\n",
    "#### Why ConvNets over Feed-Forward Neural Nets?\n",
    "\n",
    "| ![Feed Forward](feedf.png) | \n",
    "|:--:| \n",
    "| *Flattening of a 3x3 image matrix into a 9x1 vector* |\n",
    "\n",
    "\n",
    "An image is nothing but a matrix of pixel values, right? So why not just flatten the image (e.g. 3x3 image matrix into a 9x1 vector) and feed it to a Multi-Level Perceptron for classification purposes? Uh.. not really.\n",
    "In cases of extremely basic binary images, the method might show an average precision score while performing prediction of classes but would have little to no accuracy when it comes to complex images having pixel dependencies throughout.\n",
    "A ConvNet is able to successfully capture the Spatial and Temporal dependencies in an image through the application of relevant filters. The architecture performs a better fitting to the image dataset due to the reduction in the number of parameters involved and reusability of weights. In other words, the network can be trained to understand the sophistication of the image better.\n",
    "\n",
    "\n",
    "#### Input Image\n",
    "\n",
    "\n",
    "| ![Input Image](inputimg.png) | \n",
    "|:--:| \n",
    "| *4x4x3 RGB Image* |\n",
    "\n",
    "In the figure, we have an RGB image which has been separated by its three color planes — Red, Green, and Blue. There are a number of such color spaces in which images exist — Grayscale, RGB, HSV, CMYK, etc.\n",
    "You can imagine how computationally intensive things would get once the images reach dimensions, say 8K (7680×4320). The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction. This is important when we are to design an architecture which is not only good at learning features but also is scalable to massive datasets.\n",
    "\n",
    "#### Convolution Layer — The Kernel\n",
    "\n",
    "| ![Covolution Layer](cl.gif) | \n",
    "|:--:| \n",
    "| *Convoluting a 5x5x1 image with a 3x3x1 kernel to get a 3x3x1 convolved feature* |\n",
    "\n",
    "Image Dimensions = 5 (Height) x 5 (Breadth) x 1 (Number of channels, eg. RGB)\n",
    "In the above demonstration, the green section resembles our 5x5x1 input image, I. The element involved in carrying out the convolution operation in the first part of a Convolutional Layer is called the Kernel/Filter, K, represented in the color yellow. We have selected K as a 3x3x1 matrix.\n",
    "\n",
    "The Kernel shifts 9 times because of Stride Length = 1 (Non-Strided), every time performing a matrix multiplication operation between K and the portion P of the image over which the kernel is hovering.\n",
    "\n",
    "\n",
    "| ![Movement of Kernel](mkl.png) | \n",
    "|:--:| \n",
    "| *Movement of Kernel* |\n",
    "\n",
    "\n",
    "The filter moves to the right with a certain Stride Value till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride Value and repeats the process until the entire image is traversed.\n",
    "\n",
    "\n",
    "| ![Covolution Operation](co.gif) | \n",
    "|:--:| \n",
    "| *Convolution operation on a MxNx3 image matrix with a 3x3x3 Kernel* |\n",
    "\n",
    "\n",
    "In the case of images with multiple channels (e.g. RGB), the Kernel has the same depth as that of the input image. Matrix Multiplication is performed between Kn and In stack ([K1, I1]; [K2, I2]; [K3, I3]) and all the results are summed with the bias to give us a squashed one-depth channel Convoluted Feature Output.\n",
    "\n",
    "\n",
    "The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer. Conventionally, the first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would.\n",
    "\n",
    "\n",
    "| ![Covolution Operation](co2.gif) | \n",
    "|:--:| \n",
    "| *Convolution Operation with Stride Length = 2* |\n",
    "\n",
    "There are two types of results to the operation — one in which the convolved feature is reduced in dimensionality as compared to the input, and the other in which the dimensionality is either increased or remains the same. This is done by applying Valid Padding in case of the former, or Same Padding in the case of the latter.\n",
    "\n",
    "When we augment the 5x5x1 image into a 6x6x1 image and then apply the 3x3x1 kernel over it, we find that the convolved matrix turns out to be of dimensions 5x5x1. Hence the name — Same Padding.\n",
    "On the other hand, if we perform the same operation without padding, we are presented with a matrix which has dimensions of the Kernel (3x3x1) itself — Valid Padding.\n",
    "The following repository houses many such GIFs which would help you get a better understanding of how Padding and Stride Length work together to achieve results relevant to our needs.\n",
    "\n",
    "\n",
    "| ![Covolution Operation](co3.gif) | \n",
    "|:--:| \n",
    "| *SAME padding: 5x5x1 image is padded with 0s to create a 6x6x1 image* |\n",
    "\n",
    "\n",
    "#### Pooling Layer\n",
    "\n",
    "Similar to the Convolutional Layer, the Pooling layer is responsible for reducing the spatial size of the Convolved Feature. This is to decrease the computational power required to process the data through dimensionality reduction. Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.\n",
    "\n",
    "\n",
    "| ![Pooling](p1.gif) | \n",
    "|:--:| \n",
    "| *3x3 pooling over 5x5 convolved feature* |\n",
    "\n",
    "\n",
    "There are two types of Pooling: Max Pooling and Average Pooling. Max Pooling returns the maximum value from the portion of the image covered by the Kernel. On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel.\n",
    "\n",
    "Max Pooling also performs as a Noise Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction. On the other hand, Average Pooling simply performs dimensionality reduction as a noise suppressing mechanism. Hence, we can say that Max Pooling performs a lot better than Average Pooling.\n",
    "\n",
    "The Convolutional Layer and the Pooling Layer, together form the i-th layer of a Convolutional Neural Network. Depending on the complexities in the images, the number of such layers may be increased for capturing low-levels details even further, but at the cost of more computational power.\n",
    "After going through the above process, we have successfully enabled the model to understand the features. Moving on, we are going to flatten the final output and feed it to a regular Neural Network for classification purposes.\n",
    "\n",
    "\n",
    "| ![Max Pooling](mp.png) | \n",
    "|:--:| \n",
    "| *Max Pooling* |\n",
    "\n",
    "\n",
    "#### Classification — Fully Connected Layer (FC Layer)\n",
    "\n",
    "\n",
    "| ![Fully COnnected Layer](fl.jpeg) | \n",
    "|:--:| \n",
    "| *Fully Connected Layer* |\n",
    "\n",
    "\n",
    "Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.\n",
    "\n",
    "Now that we have converted our input image into a suitable form for our Multi-Level Perceptron, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique.\n",
    "There are various architectures of CNNs available which have been key in building algorithms which power and shall power AI as a whole in the foreseeable future. Some of them have been listed below:\n",
    "    \n",
    "    1. LeNet\n",
    "    2. AlexNet\n",
    "    3. VGGNet\n",
    "    4. GoogLeNet\n",
    "    5. ResNet\n",
    "    6. ZFNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "\n",
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIleuCAjoFD8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply some Geometric Transformations on Training Set images to make them a bit different from each other to increase the diversity of the Training set and avoid overfitting.\n",
    "\n",
    "The process of applying series of Geometric Transformation on image Dataset is called Image Augmentation and output of this process are augmented image.\n",
    "\n",
    "First of all to augment our image using the ImageDataGenerator class of image module of preprocessing library of keras package\n",
    "we create an object or instance of ImageDataGenerator class wich certain arguments which apply transformartions.\n",
    "\n",
    "After then we apply this ImageDataGenerator object train_datagen to our training set images to get augmented training set.\n",
    "\n",
    "The know more about each pararmeters these image check [kearas API](https://keras.io/api/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7999 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Test set we only apply pixel rescalling transformation and not the Geometric Transformation as this dataset contain images whcih simulate the real world scenario and can't be tempered.\n",
    "\n",
    "And as similar as above we create an ImageDataGenearator object to apply rescaling transformation to our test set and then connect it our test set to perform the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Refer Keras API to know about each* [arguments](https://keras.io/api/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compile our ANN we are going to use compile method of tensorflow which take as argument:\n",
    "\n",
    "    optimizer: a method which optimizes the weight on edge between neurons of two layers with aim of reducing the loss\n",
    "\n",
    "    loss: loss is difference between predicted and actual value\n",
    "\n",
    "    metrics: is the array which takes different parameters which are used to measure the efficicency of ANN\n",
    "\n",
    "In this case we are using 'adam' optimizer fuction which is an Stochastic Gradient Descent and 'binary_crossentropy' as our loss funciton and we are just measuring the accuracy of the ANN .\n",
    "\n",
    "In this case we are basically classifying user wheter he exited or not thus binary classification and binary_crossentropy loss fucnction.\n",
    "\n",
    "If we have multiple classes to classify we will use 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our CNN we are using fit method of sklearn which takes as agument feature train matrix and target train vector as well as the size of batch we are training and iterations or epochs or number of iter to train.\n",
    "\n",
    "In ANN we dont train the model on whole dataset at once instead we supply the model with batch of training data and its values is experimental but in most of the cases 32 suits.\n",
    "\n",
    "Epochs is the number of iterations to do training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 111s 445ms/step - loss: 0.6818 - accuracy: 0.5684 - val_loss: 0.6369 - val_accuracy: 0.6615\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 48s 193ms/step - loss: 0.6155 - accuracy: 0.6611 - val_loss: 0.5773 - val_accuracy: 0.7045\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.5733 - accuracy: 0.7001 - val_loss: 0.5368 - val_accuracy: 0.7360\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.5350 - accuracy: 0.7268 - val_loss: 0.5167 - val_accuracy: 0.7480\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.5116 - accuracy: 0.7431 - val_loss: 0.5310 - val_accuracy: 0.7320\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.4832 - accuracy: 0.7623 - val_loss: 0.5014 - val_accuracy: 0.7585\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.4591 - accuracy: 0.7835 - val_loss: 0.4909 - val_accuracy: 0.7685\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.4451 - accuracy: 0.7926 - val_loss: 0.5423 - val_accuracy: 0.7550\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.4242 - accuracy: 0.7997 - val_loss: 0.4687 - val_accuracy: 0.7885\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.4048 - accuracy: 0.8180 - val_loss: 0.4579 - val_accuracy: 0.7965\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.3970 - accuracy: 0.8191 - val_loss: 0.4605 - val_accuracy: 0.7955\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.3793 - accuracy: 0.8256 - val_loss: 0.4862 - val_accuracy: 0.7935\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.3665 - accuracy: 0.8310 - val_loss: 0.4673 - val_accuracy: 0.7995\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.3541 - accuracy: 0.8436 - val_loss: 0.4689 - val_accuracy: 0.8020\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 0.3327 - accuracy: 0.8551 - val_loss: 0.4881 - val_accuracy: 0.8075\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.3270 - accuracy: 0.8585 - val_loss: 0.4638 - val_accuracy: 0.8020\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.3004 - accuracy: 0.8719 - val_loss: 0.4937 - val_accuracy: 0.7935\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.2985 - accuracy: 0.8731 - val_loss: 0.4957 - val_accuracy: 0.8020\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.2799 - accuracy: 0.8770 - val_loss: 0.4911 - val_accuracy: 0.8005\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.2744 - accuracy: 0.8799 - val_loss: 0.5419 - val_accuracy: 0.7965\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.2436 - accuracy: 0.8991 - val_loss: 0.4934 - val_accuracy: 0.8035\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.2425 - accuracy: 0.9001 - val_loss: 0.5402 - val_accuracy: 0.8050\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 0.2244 - accuracy: 0.9087 - val_loss: 0.5232 - val_accuracy: 0.8055\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 0.2115 - accuracy: 0.9147 - val_loss: 0.5775 - val_accuracy: 0.7945\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.2085 - accuracy: 0.9157 - val_loss: 0.6138 - val_accuracy: 0.7885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x251782695e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the image using the load_img method which returns image in PIL format.\n",
    "After that we convert this PIL image into a numpy array using the img_to_array method.\n",
    "\n",
    "As we are training the CNN using the batch of datasets which add an extra dimension to array and to add this dimension to our test image array we are using the expand_dims method which takes the image and index of dimension as argument.\n",
    "\n",
    "After that we pass our array image to predict method and then get the indices of our training set.\n",
    "\n",
    "To get the result in preetified human undestandable manner we have to acess the output from the result output array as images  were in batch thus the ouptuts are also in batches and batches have index 0 so we first acess a batch and after that the prediction within that batch is also at first index which is agained acess by the index 0.\n",
    "\n",
    "And after that we assign name of that category according to binary prediction and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ED9KB3I54c1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
