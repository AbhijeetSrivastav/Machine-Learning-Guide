{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word Model - NLP- Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "### The Problem with Text\n",
    "\n",
    "A problem with modeling text is that it is messy, and techniques like machine learning algorithms prefer well defined fixed-length inputs and outputs.\n",
    "\n",
    "Machine learning algorithms cannot work with raw text directly; the text must be converted into numbers. Specifically, vectors of numbers.\n",
    "\n",
    "In language processing, the vectors x are derived from textual data, in order to reflect various linguistic properties of the text.\n",
    "\n",
    "This is called feature extraction or feature encoding.\n",
    "\n",
    "A popular and simple method of feature extraction with text data is called the bag-of-words model of text.\n",
    "\n",
    "### What is a Bag-of-Words?\n",
    "\n",
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
    "\n",
    "The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "A vocabulary of known words.\n",
    "\n",
    "A measure of the presence of known words.\n",
    "\n",
    "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
    "\n",
    "A very common feature extraction procedures for sentences and documents is the bag-of-words approach (BOW). In this approach, we look at the histogram of the words within the text, i.e. considering each word count as a feature.\n",
    "\n",
    "The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document.\n",
    "\n",
    "The bag-of-words can be as simple or complex as you like. The complexity comes both in deciding how to design the vocabulary of known words (or tokens) and how to score the presence of known words.\n",
    "\n",
    "We will take a closer look at both of these concerns.\n",
    "\n",
    "### Example of the Bag-of-Words Model\n",
    "\n",
    "Let’s make the bag-of-words model concrete with a worked example.\n",
    "\n",
    "##### Step 1: Collect Data\n",
    "Below is a snippet of the first few lines of text from the book “A Tale of Two Cities” by Charles Dickens, taken from Project Gutenberg.\n",
    "\n",
    "It was the best of times,\n",
    "it was the worst of times,\n",
    "it was the age of wisdom,\n",
    "it was the age of foolishness,\n",
    "\n",
    "For this small example, let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents.\n",
    "\n",
    "##### Step 2: Design the Vocabulary\n",
    "Now we can make a list of all of the words in our model vocabulary.\n",
    "\n",
    "The unique words here (ignoring case and punctuation) are:\n",
    "\n",
    "“it”\n",
    "“was”\n",
    "“the”\n",
    "“best”\n",
    "“of”\n",
    "“times”\n",
    "“worst”\n",
    "“age”\n",
    "“wisdom”\n",
    "“foolishness”\n",
    "That is a vocabulary of 10 words from a corpus containing 24 words.\n",
    "\n",
    "##### Step 3: Create Document Vectors\n",
    "The next step is to score the words in each document.\n",
    "\n",
    "The objective is to turn each document of free text into a vector that we can use as input or output for a machine learning model.\n",
    "\n",
    "Because we know the vocabulary has 10 words, we can use a fixed-length document representation of 10, with one position in the vector to score each word.\n",
    "\n",
    "The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.\n",
    "\n",
    "Using the arbitrary ordering of words listed above in our vocabulary, we can step through the first document (“It was the best of times“) and convert it into a binary vector.\n",
    "\n",
    "The scoring of the document would look as follows:\n",
    "\n",
    "“it” = 1\n",
    "“was” = 1\n",
    "“the” = 1\n",
    "“best” = 1\n",
    "“of” = 1\n",
    "“times” = 1\n",
    "“worst” = 0\n",
    "“age” = 0\n",
    "“wisdom” = 0\n",
    "“foolishness” = 0\n",
    "As a binary vector, this would look as follows:\n",
    "\n",
    "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "1\n",
    "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "The other three documents would look as follows:\n",
    "\n",
    "\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
    "\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
    "\n",
    "All ordering of the words is nominally discarded and we have a consistent way of extracting features from any document in our corpus, ready for use in modeling.\n",
    "\n",
    "New documents that overlap with the vocabulary of known words, but may contain words outside of the vocabulary, can still be encoded, where only the occurrence of known words are scored and unknown words are ignored.\n",
    "\n",
    "You can see how this might naturally scale to large vocabularies and larger documents.\n",
    "\n",
    "### Managing Vocabulary\n",
    "\n",
    "As the vocabulary size increases, so does the vector representation of documents.\n",
    "\n",
    "In the previous example, the length of the document vector is equal to the number of known words.\n",
    "\n",
    "You can imagine that for a very large corpus, such as thousands of books, that the length of the vector might be thousands or millions of positions. Further, each document may contain very few of the known words in the vocabulary.\n",
    "\n",
    "This results in a vector with lots of zero scores, called a sparse vector or sparse representation.\n",
    "\n",
    "Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms.\n",
    "\n",
    "As such, there is pressure to decrease the size of the vocabulary when using a bag-of-words model.\n",
    "\n",
    "There are simple text cleaning techniques that can be used as a first step, such as:\n",
    "\n",
    "Ignoring case\n",
    "Ignoring punctuation\n",
    "Ignoring frequent words that don’t contain much information, called stop words, like “a,” “of,” etc.\n",
    "Fixing misspelled words.\n",
    "Reducing words to their stem (e.g. “play” from “playing”) using stemming algorithms.\n",
    "A more sophisticated approach is to create a vocabulary of grouped words. This both changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more meaning from the document.\n",
    "\n",
    "In this approach, each word or token is called a “gram”. Creating a vocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that appear in the corpus are modeled, not all possible bigrams.\n",
    "\n",
    "An N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “please turn”, “turn your”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”.\n",
    "\n",
    "For example, the bigrams in the first line of text in the previous section: “It was the best of times” are as follows:\n",
    "\n",
    "“it was”\n",
    "“was the”\n",
    "“the best”\n",
    "“best of”\n",
    "“of times”\n",
    "A vocabulary then tracks triplets of words is called a trigram model and the general approach is called the n-gram model, where n refers to the number of grouped words.\n",
    "\n",
    "Often a simple bigram approach is better than a 1-gram bag-of-words model for tasks like documentation classification.\n",
    "\n",
    "a bag-of-bigrams representation is much more powerful than bag-of-words, and in many cases proves very hard to beat.\n",
    "\n",
    "— Page 75, Neural Network Methods in Natural Language Processing, 2017.\n",
    "\n",
    "### Scoring Words\n",
    "\n",
    "Once a vocabulary has been chosen, the occurrence of words in example documents needs to be scored.\n",
    "\n",
    "In the worked example, we have already seen one very simple approach to scoring: a binary scoring of the presence or absence of words.\n",
    "\n",
    "Some additional simple scoring methods include:\n",
    "\n",
    "Counts. Count the number of times each word appears in a document.\n",
    "Frequencies. Calculate the frequency that each word appears in a document out of all the words in the document.\n",
    "Word Hashing\n",
    "You may remember from computer science that a hash function is a bit of math that maps data to a fixed size set of numbers.\n",
    "\n",
    "For example, we use them in hash tables when programming where perhaps names are converted to numbers for fast lookup.\n",
    "\n",
    "We can use a hash representation of known words in our vocabulary. This addresses the problem of having a very large vocabulary for a large text corpus because we can choose the size of the hash space, which is in turn the size of the vector representation of the document.\n",
    "\n",
    "Words are hashed deterministically to the same integer index in the target hash space. A binary score or count can then be used to score the word.\n",
    "\n",
    "This is called the “hash trick” or “feature hashing“.\n",
    "\n",
    "The challenge is to choose a hash space to accommodate the chosen vocabulary size to minimize the probability of collisions and trade-off sparsity.\n",
    "\n",
    "### Limitations of Bag-of-Words\n",
    "\n",
    "The bag-of-words model is very simple to understand and implement and offers a lot of flexibility for customization on your specific text data.\n",
    "\n",
    "It has been used with great success on prediction problems like language modeling and documentation classification.\n",
    "\n",
    "Nevertheless, it suffers from some shortcomings, such as:\n",
    "\n",
    "1. Vocabulary: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n",
    "\n",
    "2. Sparsity: Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
    "\n",
    "3. Meaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” vs “is this interesting”), synonyms (“old bike” vs “used bike”), and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are importing a tab seperated value (tsv) dataset which is also a csv file but the seperation parameter is tab. This dataset is in tsv format becuase texts also contain commas which will lead to mal processing of data.\n",
    "\n",
    "We also need to avoid the double quotation(\"\") for this we use the quoting argument of the read_csv method of pandas and set its value to 3 which means to avoid. \n",
    "\n",
    "We also set the seperation parameter(delimiter) of read_csv method to tab(\\t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Liked\n",
      "0                             Wow... Loved this place.      1\n",
      "1                                   Crust is not good.      0\n",
      "2            Not tasty and the texture was just nasty.      0\n",
      "3    Stopped by during the late May bank holiday of...      1\n",
      "4    The selection on the menu was great and so wer...      1\n",
      "5       Now I am getting angry and I want my damn pho.      0\n",
      "6                Honeslty it didn't taste THAT fresh.)      0\n",
      "7    The potatoes were like rubber and you could te...      0\n",
      "8                            The fries were great too.      1\n",
      "9                                       A great touch.      1\n",
      "10                            Service was very prompt.      1\n",
      "11                                  Would not go back.      0\n",
      "12   The cashier had no care what so ever on what I...      0\n",
      "13   I tried the Cape Cod ravoli, chicken, with cra...      1\n",
      "14   I was disgusted because I was pretty sure that...      0\n",
      "15   I was shocked because no signs indicate cash o...      0\n",
      "16                                 Highly recommended.      1\n",
      "17              Waitress was a little slow in service.      0\n",
      "18   This place is not worth your time, let alone V...      0\n",
      "19                                did not like at all.      0\n",
      "20                                 The Burrittos Blah!      0\n",
      "21                                  The food, amazing.      1\n",
      "22                               Service is also cute.      1\n",
      "23   I could care less... The interior is just beau...      1\n",
      "24                                  So they performed.      1\n",
      "25   That's right....the red velvet cake.....ohhh t...      1\n",
      "26          - They never brought a salad we asked for.      0\n",
      "27   This hole in the wall has great Mexican street...      1\n",
      "28   Took an hour to get our food only 4 tables in ...      0\n",
      "29                   The worst was the salmon sashimi.      0\n",
      "..                                                 ...    ...\n",
      "970  I immediately said I wanted to talk to the man...      0\n",
      "971                    The ambiance isn't much better.      0\n",
      "972  Unfortunately, it only set us up for disapppoi...      0\n",
      "973                              The food wasn't good.      0\n",
      "974  Your servers suck, wait, correction, our serve...      0\n",
      "975      What happened next was pretty....off putting.      0\n",
      "976  too bad cause I know it's family owned, I real...      0\n",
      "977               Overpriced for what you are getting.      0\n",
      "978               I vomited in the bathroom mid lunch.      0\n",
      "979  I kept looking at the time and it had soon bec...      0\n",
      "980  I have been to very few places to eat that und...      0\n",
      "981  We started with the tuna sashimi which was bro...      0\n",
      "982                            Food was below average.      0\n",
      "983  It sure does beat the nachos at the movies but...      0\n",
      "984       All in all, Ha Long Bay was a bit of a flop.      0\n",
      "985  The problem I have is that they charge $11.99 ...      0\n",
      "986  Shrimp- When I unwrapped it (I live only 1/2 a...      0\n",
      "987     It lacked flavor, seemed undercooked, and dry.      0\n",
      "988  It really is impressive that the place hasn't ...      0\n",
      "989  I would avoid this place if you are staying in...      0\n",
      "990  The refried beans that came with my meal were ...      0\n",
      "991         Spend your money and time some place else.      0\n",
      "992  A lady at the table next to us found a live gr...      0\n",
      "993            the presentation of the food was awful.      0\n",
      "994           I can't tell you how disappointed I was.      0\n",
      "995  I think food should have flavor and texture an...      0\n",
      "996                           Appetite instantly gone.      0\n",
      "997  Overall I was not impressed and would not go b...      0\n",
      "998  The whole experience was underwhelming, and I ...      0\n",
      "999  Then, as if I hadn't wasted enough of my life ...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cleaning the Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we download the Stop Words from NLTK library.\n",
    "Stop words are those words which dont convey any sentiment (ex -  is, am, are etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to import the stopwords from nltk library which we downloaded earlier.\n",
    "\n",
    "Now we will loop through each entries(reviews) of dataset using the for loop and for each entries we will do:\n",
    "    \n",
    "    remove every charachter except a to z and A to Z for this we are going to use the sub method of the re library which \n",
    "    takes a regurlar expression to specify the filter and as another argument takes the charchter with which it will \n",
    "    replace the charachter not specified in filter and last argument it takes is the data on which to apply filter, for \n",
    "    applying the filter we will acess the review from dataset by specifying the column of datset and index of the data.\n",
    "    \n",
    "    After this it converts all the data(reviews) from dataset to lower case using the lower function.\n",
    "    \n",
    "    And at last we seperate each charachter of each data(review) using the split method.\n",
    "    \n",
    "    Now we will apply stemming on each word of review using the stem method except the words which are stop words and for this we will iterate trough a single review.\n",
    "    \n",
    "    After that we will join each stemmed words by space.\n",
    "    \n",
    "    Then we will append our corpus list with processed review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps =  PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our corpus words to toke using the CountVectorizer method of feature_extraction.text module of sklearn library. It will return a Sparse Matrix as ouptut.\n",
    "\n",
    "But their are still few words in corpus which dont have any sentiments and were not present in stop words list thus were not removed in previous steps.\n",
    "\n",
    "To remove suc word we provide our CounVectorizer object max_features argument which take the value of how much word to include for training.\n",
    "\n",
    "To get the value of max_feature first of all we run the below code without providing this argument and getting its value by len(X[O]) and after that passing the argument with found value.\n",
    "\n",
    "Now we train our model using fit_transform method on corpus and convert the output to a 2D array.\n",
    "\n",
    "After that we get our dependent variable which is already present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset in Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
